{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ Netflix Recommendation System - ULTIMATE OPTIMIZED MODEL\n",
    "\n",
    "## üöÄ Maximum Performance Optimization\n",
    "\n",
    "**Problem:** Current model shows 77.9% poor performance (similarity < 40%)\n",
    "\n",
    "**Goal:** Achieve 50-60% average similarity with < 30% poor recommendations\n",
    "\n",
    "### Key Optimizations:\n",
    "1. ‚úÖ **Increased features**: 3000 ‚Üí 5000 (better vocabulary)\n",
    "2. ‚úÖ **Trigrams included**: (1,3) instead of (1,2) for better context\n",
    "3. ‚úÖ **Aggressive weighting**: Genre 4x, Description 3x\n",
    "4. ‚úÖ **Better filtering**: Optimized min_df and max_df\n",
    "5. ‚úÖ **Enhanced preprocessing**: Multiple cleaning passes\n",
    "6. ‚úÖ **Country/Rating features**: Added for better matching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nüéØ Target: Average similarity > 50%\")\n",
    "print(\"üéØ Target: Poor recommendations < 30%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df = pd.read_csv('netflix_cleaned.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} titles\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"‚úÖ Index reset\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nüìä Missing values:\")\n",
    "for col in df.columns:\n",
    "    missing = df[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"   {col}: {missing} ({missing/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: ULTIMATE Feature Engineering\n",
    "\n",
    "### Revolutionary Improvements:\n",
    "1. **Multi-pass text cleaning** - Remove noise thoroughly\n",
    "2. **Aggressive feature weighting**:\n",
    "   - Genre: 4x weight (most critical)\n",
    "   - Description: 3x weight (very important)\n",
    "   - Director: 2x weight\n",
    "   - Cast: 2x weight\n",
    "   - Country: 2x weight (NEW - helps regional matching)\n",
    "   - Rating: 1x weight (NEW - helps age-appropriate matching)\n",
    "3. **Enhanced text preprocessing** - Better normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Multi-pass advanced text cleaning\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == 'Unknown':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep spaces and alphanumeric\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers (optional - uncomment if you want)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def create_ultimate_features(row):\n",
    "    \"\"\"\n",
    "    Create ultimate feature string with AGGRESSIVE weighting:\n",
    "    - Genre: 4x weight (MOST CRITICAL for similarity)\n",
    "    - Description: 3x weight (Very important for content)\n",
    "    - Director: 2x weight (Important for style)\n",
    "    - Cast: 2x weight (Important for type)\n",
    "    - Country: 2x weight (NEW - Regional/cultural matching)\n",
    "    - Rating: 1x weight (NEW - Age-appropriate matching)\n",
    "    - Type: 1x weight (Movie vs TV Show)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean each component with advanced cleaning\n",
    "    genre = advanced_text_cleaning(row['listed_in'])\n",
    "    description = advanced_text_cleaning(row['description'])\n",
    "    director = advanced_text_cleaning(row['director'])\n",
    "    cast = advanced_text_cleaning(row['cast'])\n",
    "    country = advanced_text_cleaning(row.get('country', ''))\n",
    "    rating = advanced_text_cleaning(row.get('rating', ''))\n",
    "    content_type = advanced_text_cleaning(row['type'])\n",
    "    \n",
    "    # Apply aggressive weighting\n",
    "    genre_weighted = ' '.join([genre] * 4)  # 4x weight!\n",
    "    description_weighted = ' '.join([description] * 3)  # 3x weight!\n",
    "    director_weighted = ' '.join([director] * 2)  # 2x weight\n",
    "    cast_weighted = ' '.join([cast] * 2)  # 2x weight\n",
    "    country_weighted = ' '.join([country] * 2)  # 2x weight (NEW!)\n",
    "    \n",
    "    # Combine all features with spacing\n",
    "    combined = (\n",
    "        f\"{genre_weighted} \"\n",
    "        f\"{description_weighted} \"\n",
    "        f\"{director_weighted} \"\n",
    "        f\"{cast_weighted} \"\n",
    "        f\"{country_weighted} \"\n",
    "        f\"{rating} \"\n",
    "        f\"{content_type}\"\n",
    "    )\n",
    "    \n",
    "    return combined.strip()\n",
    "\n",
    "print(\"üîß Creating ULTIMATE enhanced features...\")\n",
    "print(\"This will take 30-60 seconds...\\n\")\n",
    "\n",
    "df['ultimate_features'] = df.apply(create_ultimate_features, axis=1)\n",
    "\n",
    "print(\"‚úÖ Ultimate features created!\")\n",
    "print(f\"\\nüìù Sample ultimate feature (first 300 chars):\")\n",
    "print(\"-\" * 70)\n",
    "print(df['ultimate_features'].iloc[0][:300] + \"...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Check feature length distribution\n",
    "feature_lengths = df['ultimate_features'].str.len()\n",
    "print(f\"\\nüìä Feature Length Stats:\")\n",
    "print(f\"   Average: {feature_lengths.mean():.0f} characters\")\n",
    "print(f\"   Median:  {feature_lengths.median():.0f} characters\")\n",
    "print(f\"   Min:     {feature_lengths.min():.0f} characters\")\n",
    "print(f\"   Max:     {feature_lengths.max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: MAXIMUM TF-IDF Optimization\n",
    "\n",
    "### Ultimate Optimizations:\n",
    "- **max_features=5000** (up from 3000 - maximum granularity)\n",
    "- **ngram_range=(1,3)** (trigrams! - captures 3-word phrases)\n",
    "- **min_df=2** (remove ultra-rare terms)\n",
    "- **max_df=0.75** (more aggressive common term removal)\n",
    "- **sublinear_tf=True** (logarithmic term frequency scaling)\n",
    "- **smooth_idf=True** (prevents zero divisions)\n",
    "- **use_idf=True** (apply inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Creating MAXIMUM OPTIMIZED TF-IDF matrix...\")\n",
    "print(\"This will take 2-3 minutes...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize ULTIMATE TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,          # üî• INCREASED! 3000 ‚Üí 5000\n",
    "    stop_words='english',       # Remove common English words\n",
    "    ngram_range=(1, 3),         # üî• TRIGRAMS! (1,2) ‚Üí (1,3)\n",
    "    min_df=2,                   # Word must appear in ‚â•2 documents\n",
    "    max_df=0.75,                # üî• MORE AGGRESSIVE! 0.8 ‚Üí 0.75\n",
    "    sublinear_tf=True,          # Use log scaling for TF\n",
    "    smooth_idf=True,            # Smooth IDF weights\n",
    "    use_idf=True,               # Use inverse document frequency\n",
    "    norm='l2',                  # L2 normalization\n",
    "    strip_accents='unicode'     # Remove accents\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print(\"\\n‚è≥ Fitting TF-IDF vectorizer...\")\n",
    "tfidf_matrix = tfidf.fit_transform(df['ultimate_features'])\n",
    "\n",
    "print(f\"\\n‚úÖ TF-IDF matrix created!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Shape:            {tfidf_matrix.shape}\")\n",
    "print(f\"   Total features:   {len(tfidf.get_feature_names_out()):,}\")\n",
    "print(f\"   Sparsity:         {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))*100:.2f}%\")\n",
    "print(f\"   Non-zero elements: {tfidf_matrix.nnz:,}\")\n",
    "print(f\"   Memory usage:     {tfidf_matrix.data.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Show sample features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"\\nüìù Sample extracted features (first 20):\")\n",
    "print(\"   \", list(feature_names[:20]))\n",
    "print(f\"\\nüìù Sample trigrams (if any):\")\n",
    "trigrams = [f for f in feature_names if len(f.split()) == 3][:10]\n",
    "print(\"   \", trigrams if trigrams else \"[Computing...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ Computing OPTIMIZED cosine similarity matrix...\")\n",
    "print(\"This may take 2-3 minutes for ~8000 titles...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Similarity matrix computed in {elapsed_time:.1f} seconds!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Shape:           {cosine_sim.shape}\")\n",
    "print(f\"   Total pairs:     {(cosine_sim.shape[0] * cosine_sim.shape[1]):,}\")\n",
    "print(f\"   Memory usage:    {cosine_sim.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "# Analyze similarity distribution (excluding diagonal)\n",
    "mask = np.ones(cosine_sim.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "off_diagonal = cosine_sim[mask]\n",
    "\n",
    "print(f\"\\nüìä Similarity Statistics (excluding self-similarity):\")\n",
    "print(f\"   Min:             {off_diagonal.min():.4f}\")\n",
    "print(f\"   Max:             {off_diagonal.max():.4f}\")\n",
    "print(f\"   Mean:            {off_diagonal.mean():.4f} ({off_diagonal.mean()*100:.2f}%)\")\n",
    "print(f\"   Median:          {np.median(off_diagonal):.4f} ({np.median(off_diagonal)*100:.2f}%)\")\n",
    "print(f\"   Std Dev:         {off_diagonal.std():.4f}\")\n",
    "\n",
    "# Percentiles\n",
    "print(f\"\\nüìä Percentiles:\")\n",
    "for p in [25, 50, 75, 90, 95, 99]:\n",
    "    val = np.percentile(off_diagonal, p)\n",
    "    print(f\"   {p}th percentile: {val:.4f} ({val*100:.2f}%)\")\n",
    "\n",
    "# Distribution\n",
    "print(f\"\\nüìä Similarity Distribution:\")\n",
    "print(f\"   ‚â•80%:  {(off_diagonal >= 0.8).sum():,} pairs ({(off_diagonal >= 0.8).sum()/len(off_diagonal)*100:.2f}%)\")\n",
    "print(f\"   60-80%: {((off_diagonal >= 0.6) & (off_diagonal < 0.8)).sum():,} pairs ({((off_diagonal >= 0.6) & (off_diagonal < 0.8)).sum()/len(off_diagonal)*100:.2f}%)\")\n",
    "print(f\"   40-60%: {((off_diagonal >= 0.4) & (off_diagonal < 0.6)).sum():,} pairs ({((off_diagonal >= 0.4) & (off_diagonal < 0.6)).sum()/len(off_diagonal)*100:.2f}%)\")\n",
    "print(f\"   20-40%: {((off_diagonal >= 0.2) & (off_diagonal < 0.4)).sum():,} pairs ({((off_diagonal >= 0.2) & (off_diagonal < 0.4)).sum()/len(off_diagonal)*100:.2f}%)\")\n",
    "print(f\"   <20%:  {(off_diagonal < 0.2).sum():,} pairs ({(off_diagonal < 0.2).sum()/len(off_diagonal)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Index Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional mappings\n",
    "title_to_index = pd.Series(df.index, index=df['title']).to_dict()\n",
    "index_to_title = pd.Series(df['title'], index=df.index).to_dict()\n",
    "\n",
    "print(f\"‚úÖ Index mappings created!\")\n",
    "print(f\"   Total titles indexed: {len(title_to_index):,}\")\n",
    "print(f\"   Sample titles: {list(df['title'].head(3).values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Enhanced Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, n=10, min_similarity=0.0):\n",
    "    \"\"\"\n",
    "    Get top N recommendations for a given title\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    title : str\n",
    "        Title of the movie/show\n",
    "    n : int\n",
    "        Number of recommendations to return\n",
    "    min_similarity : float\n",
    "        Minimum similarity threshold (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with recommendations and similarity scores\n",
    "    \"\"\"\n",
    "    if title not in title_to_index:\n",
    "        print(f\"‚ùå '{title}' not found in database!\")\n",
    "        print(f\"\\nüí° Suggestions:\")\n",
    "        # Find similar titles\n",
    "        similar_titles = df[df['title'].str.contains(title.split()[0], case=False, na=False)]['title'].head(5)\n",
    "        if len(similar_titles) > 0:\n",
    "            for t in similar_titles:\n",
    "                print(f\"   ‚Ä¢ {t}\")\n",
    "        return None\n",
    "    \n",
    "    # Get index\n",
    "    idx = title_to_index[title]\n",
    "    \n",
    "    # Get similarity scores for this title\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Exclude the title itself and filter by min_similarity\n",
    "    sim_scores = [(i, score) for i, score in sim_scores[1:] if score >= min_similarity]\n",
    "    \n",
    "    # Get top N\n",
    "    sim_scores = sim_scores[:n]\n",
    "    \n",
    "    if len(sim_scores) == 0:\n",
    "        print(f\"‚ö†Ô∏è  No recommendations found with similarity ‚â• {min_similarity}\")\n",
    "        return None\n",
    "    \n",
    "    # Get movie indices\n",
    "    indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = df.iloc[indices][['title', 'type', 'release_year', 'rating', 'listed_in']].copy()\n",
    "    results['similarity_score'] = [score[1] for score in sim_scores]\n",
    "    results['similarity_pct'] = results['similarity_score'] * 100\n",
    "    \n",
    "    # Add quality category\n",
    "    results['quality'] = results['similarity_pct'].apply(\n",
    "        lambda x: 'üåü Excellent' if x >= 80 else \n",
    "                  '‚ú® Great' if x >= 60 else \n",
    "                  'üëç Good' if x >= 40 else \n",
    "                  '‚ö†Ô∏è  Fair'\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Enhanced recommendation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Comprehensive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with diverse titles\n",
    "test_titles = [\n",
    "    'Stranger Things',\n",
    "    'Breaking Bad', \n",
    "    'The Dark Knight',\n",
    "    'Friends',\n",
    "    'Inception'\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING OPTIMIZED RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for test_title in test_titles:\n",
    "    if test_title in title_to_index:\n",
    "        print(f\"\\nüì∫ Top 5 Recommendations for: {test_title}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        recs = get_recommendations(test_title, n=5)\n",
    "        \n",
    "        if recs is not None:\n",
    "            for i, (idx, row) in enumerate(recs.iterrows(), 1):\n",
    "                print(f\"\\n  {i}. {row['title']}\")\n",
    "                print(f\"     {row['quality']} - {row['similarity_pct']:.1f}% match\")\n",
    "                print(f\"     {row['type']} ‚Ä¢ {row['release_year']} ‚Ä¢ {row['rating']}\")\n",
    "                print(f\"     Genres: {row['listed_in'][:60]}...\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  '{test_title}' not found in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: MODEL PERFORMANCE ANALYSIS\n",
    "\n",
    "### This is the critical step that shows improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample 100 random titles and get their top-10 recommendations\n",
    "print(\"\\n‚è≥ Analyzing 100 sample titles (this may take 30 seconds)...\\n\")\n",
    "\n",
    "sample_titles = df['title'].sample(min(100, len(df)), random_state=42)\n",
    "all_top_scores = []\n",
    "\n",
    "for title in sample_titles:\n",
    "    recs = get_recommendations(title, n=10)\n",
    "    if recs is not None and len(recs) > 0:\n",
    "        all_top_scores.extend(recs['similarity_score'].tolist())\n",
    "\n",
    "all_top_scores = np.array(all_top_scores)\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Overall statistics\n",
    "print(f\"‚ú® Top-10 Recommendation Scores (100 sample titles):\")\n",
    "print(f\"   Average:  {all_top_scores.mean():.3f} ({all_top_scores.mean()*100:.1f}%)\")\n",
    "print(f\"   Median:   {np.median(all_top_scores):.3f} ({np.median(all_top_scores)*100:.1f}%)\")\n",
    "print(f\"   Std Dev:  {all_top_scores.std():.3f}\")\n",
    "print(f\"   Max:      {all_top_scores.max():.3f} ({all_top_scores.max()*100:.1f}%)\")\n",
    "print(f\"   Min:      {all_top_scores.min():.3f} ({all_top_scores.min()*100:.1f}%)\")\n",
    "\n",
    "# Quality distribution\n",
    "print(f\"\\nüìà Quality Distribution:\")\n",
    "excellent = (all_top_scores >= 0.80).sum()\n",
    "great = ((all_top_scores >= 0.60) & (all_top_scores < 0.80)).sum()\n",
    "good = ((all_top_scores >= 0.40) & (all_top_scores < 0.60)).sum()\n",
    "poor = (all_top_scores < 0.40).sum()\n",
    "total = len(all_top_scores)\n",
    "\n",
    "print(f\"   Excellent (‚â•80%): {excellent:4d} ({excellent/total*100:5.1f}%)\")\n",
    "print(f\"   Great (60-79%):   {great:4d} ({great/total*100:5.1f}%)\")\n",
    "print(f\"   Good (40-59%):    {good:4d} ({good/total*100:5.1f}%)\")\n",
    "print(f\"   Poor (<40%):      {poor:4d} ({poor/total*100:5.1f}%)\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "avg_pct = all_top_scores.mean() * 100\n",
    "poor_pct = poor / total * 100\n",
    "\n",
    "if avg_pct >= 50.0:\n",
    "    print(f\"\\n‚úÖ ‚úÖ ‚úÖ EXCELLENT! Average similarity is {avg_pct:.1f}% (Target: >50%)\")\n",
    "elif avg_pct >= 45.0:\n",
    "    print(f\"\\n‚úÖ ‚úÖ GREAT! Average similarity is {avg_pct:.1f}% (Close to target)\")\n",
    "elif avg_pct >= 40.0:\n",
    "    print(f\"\\n‚úÖ GOOD! Average similarity is {avg_pct:.1f}% (Improved from 32.4%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Still needs work. Average similarity is {avg_pct:.1f}%\")\n",
    "\n",
    "if poor_pct <= 30.0:\n",
    "    print(f\"‚úÖ ‚úÖ ‚úÖ EXCELLENT! Only {poor_pct:.1f}% poor recommendations (Target: <30%)\")\n",
    "elif poor_pct <= 40.0:\n",
    "    print(f\"‚úÖ ‚úÖ GREAT! {poor_pct:.1f}% poor recommendations (Better than 77.9%)\")\n",
    "elif poor_pct <= 50.0:\n",
    "    print(f\"‚úÖ GOOD! {poor_pct:.1f}% poor recommendations (Significant improvement)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  {poor_pct:.1f}% poor recommendations (needs more tuning)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare to baseline\n",
    "print(f\"\\nüìä Comparison to Baseline (Original Model):\")\n",
    "print(f\"   Average Score: 32.4% ‚Üí {avg_pct:.1f}% ({avg_pct - 32.4:+.1f}%)\")\n",
    "print(f\"   Poor (<40%):   77.9% ‚Üí {poor_pct:.1f}% ({poor_pct - 77.9:+.1f}%)\")\n",
    "print(f\"   Good (40-59%): 20.3% ‚Üí {good/total*100:.1f}% ({good/total*100 - 20.3:+.1f}%)\")\n",
    "print(f\"   Great (60-79%): 1.4% ‚Üí {great/total*100:.1f}% ({great/total*100 - 1.4:+.1f}%)\")\n",
    "print(f\"   Excellent (‚â•80%): 0.4% ‚Üí {excellent/total*100:.1f}% ({excellent/total*100 - 0.4:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\nüíæ SAVING OPTIMIZED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare system data with all components\n",
    "system_data = {\n",
    "    'df': df,\n",
    "    'cosine_sim': cosine_sim,\n",
    "    'tfidf_matrix': tfidf_matrix,\n",
    "    'tfidf_vectorizer': tfidf,\n",
    "    'title_to_index': title_to_index,\n",
    "    'index_to_title': index_to_title,\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'version': 'optimized_v3_ultimate',\n",
    "    'performance': {\n",
    "        'avg_similarity': float(all_top_scores.mean()),\n",
    "        'median_similarity': float(np.median(all_top_scores)),\n",
    "        'poor_percentage': float(poor / total * 100),\n",
    "        'excellent_percentage': float(excellent / total * 100)\n",
    "    },\n",
    "    'optimizations': [\n",
    "        'Max features: 5000 (up from 3000)',\n",
    "        'Trigrams: (1,3) ngrams',\n",
    "        'Aggressive weighting: Genre 4x, Description 3x',\n",
    "        'Enhanced preprocessing with multi-pass cleaning',\n",
    "        'Country and rating features added',\n",
    "        'Optimized min_df and max_df parameters'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the complete recommendation system\n",
    "filename = 'netflix_recommendation_system.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(system_data, f)\n",
    "\n",
    "file_size_mb = os.path.getsize(filename) / (1024**2)\n",
    "print(f\"‚úÖ Model saved as '{filename}'\")\n",
    "print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "# Save database\n",
    "df.to_csv('netflix_content_database.csv', index=False)\n",
    "print(f\"‚úÖ Database saved as 'netflix_content_database.csv'\")\n",
    "\n",
    "# Create a smaller version without similarity matrix (for deployment)\n",
    "compact_data = {\n",
    "    'df': df,\n",
    "    'tfidf_matrix': tfidf_matrix,\n",
    "    'tfidf_vectorizer': tfidf,\n",
    "    'title_to_index': title_to_index,\n",
    "    'index_to_title': index_to_title,\n",
    "    'version': 'compact_v3'\n",
    "}\n",
    "\n",
    "compact_filename = 'netflix_recommender_compact.pkl'\n",
    "with open(compact_filename, 'wb') as f:\n",
    "    pickle.dump(compact_data, f)\n",
    "\n",
    "compact_size_mb = os.path.getsize(compact_filename) / (1024**2)\n",
    "print(f\"‚úÖ Compact model saved as '{compact_filename}'\")\n",
    "print(f\"   File size: {compact_size_mb:.2f} MB (smaller for deployment)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ ULTIMATE OPTIMIZED MODEL COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚ú® Revolutionary Improvements:\")\n",
    "print(\"   ‚Ä¢ Genre weighted 4x (maximum importance!)\")\n",
    "print(\"   ‚Ä¢ Description weighted 3x (critical for content)\")\n",
    "print(\"   ‚Ä¢ Country & rating features added (NEW!)\")\n",
    "print(\"   ‚Ä¢ Features increased: 3000 ‚Üí 5000 (+67%)\")\n",
    "print(\"   ‚Ä¢ Trigrams added for 3-word phrase matching\")\n",
    "print(\"   ‚Ä¢ Multi-pass advanced text cleaning\")\n",
    "print(\"   ‚Ä¢ Optimized TF-IDF parameters\")\n",
    "\n",
    "print(f\"\\nüéØ Achieved Results:\")\n",
    "print(f\"   ‚Ä¢ Average similarity: {all_top_scores.mean()*100:.1f}% (Target: >50%)\")\n",
    "print(f\"   ‚Ä¢ Poor recommendations: {poor/total*100:.1f}% (Target: <30%)\")\n",
    "print(f\"   ‚Ä¢ Excellent recommendations: {excellent/total*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìà Performance vs Original:\")\n",
    "print(f\"   ‚Ä¢ Average: +{all_top_scores.mean()*100 - 32.4:.1f}% improvement\")\n",
    "print(f\"   ‚Ä¢ Poor recommendations: {77.9 - poor/total*100:.1f}% reduction\")\n",
    "\n",
    "print(\"\\nüì± Next Steps:\")\n",
    "print(\"   1. Restart your Streamlit app\")\n",
    "print(\"   2. Test with various movie titles\")\n",
    "print(\"   3. Enjoy much better recommendations! üé¨\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Feature Importance Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get feature names and their frequencies\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "feature_scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "# Create DataFrame and sort\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'score': feature_scores\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "print(f\"\\nüîù Top 20 Most Important Features:\\n\")\n",
    "for i, row in feature_df.head(20).iterrows():\n",
    "    ngram_size = len(row['feature'].split())\n",
    "    ngram_type = f\"{ngram_size}-gram\" if ngram_size > 1 else \"unigram\"\n",
    "    print(f\"   {row['feature']:30s} - Score: {row['score']:.2f} ({ngram_type})\")\n",
    "\n",
    "# Count ngram types\n",
    "unigrams = sum(1 for f in feature_names if len(f.split()) == 1)\n",
    "bigrams = sum(1 for f in feature_names if len(f.split()) == 2)\n",
    "trigrams = sum(1 for f in feature_names if len(f.split()) == 3)\n",
    "\n",
    "print(f\"\\nüìä N-gram Distribution:\")\n",
    "print(f\"   Unigrams:  {unigrams:,} ({unigrams/len(feature_names)*100:.1f}%)\")\n",
    "print(f\"   Bigrams:   {bigrams:,} ({bigrams/len(feature_names)*100:.1f}%)\")\n",
    "print(f\"   Trigrams:  {trigrams:,} ({trigrams/len(feature_names)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
